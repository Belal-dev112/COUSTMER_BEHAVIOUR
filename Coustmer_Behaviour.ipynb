{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLBMoiUsqqGDf25qDjvwJB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Belal-dev112/COUSTMER_BEHAVIOUR/blob/main/Coustmer_Behaviour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvE_y1rRx8zg",
        "outputId": "1d0c0c37-fbb6-446d-b3e5-3b6c6e2bd38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CUSTOMER CHURN PREDICTION & BEHAVIORAL ANALYTICS SYSTEM\n",
            "======================================================================\n",
            "A comprehensive end-to-end machine learning solution\n",
            "======================================================================\n",
            "\n",
            "\n",
            "[STEP 1] Generating Customer Data...\n",
            "✓ Generated 10000 customer records\n",
            "\n",
            "[STEP 2-3] Data Preprocessing and Feature Engineering...\n",
            "\n",
            "======================================================================\n",
            "DATA CLEANING\n",
            "======================================================================\n",
            "\n",
            "Missing Values Before Cleaning:\n",
            "age                    641\n",
            "total_charges          713\n",
            "feature_usage_score    683\n",
            "download_volume_gb     748\n",
            "dtype: int64\n",
            "\n",
            "monthly_charges: Capped 109 outliers\n",
            "\n",
            "total_charges: Capped 132 outliers\n",
            "\n",
            "download_volume_gb: Capped 698 outliers\n",
            "\n",
            "Missing Values After Cleaning:\n",
            "0 total missing values\n",
            "\n",
            "======================================================================\n",
            "FEATURE ENGINEERING\n",
            "======================================================================\n",
            "\n",
            "Created 32 features\n",
            "New engineered features: 14\n",
            "\n",
            "[STEP 4] Exploratory Data Analysis...\n",
            "\n",
            "======================================================================\n",
            "STATISTICAL SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Dataset Shape: (10000, 34)\n",
            "\n",
            "Churn Distribution:\n",
            "churn\n",
            "1    7774\n",
            "0    2226\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Churn Rate: 77.74%\n",
            "✓ EDA visualizations saved\n",
            "\n",
            "[STEP 5] Handling Class Imbalance...\n",
            "\n",
            "======================================================================\n",
            "HANDLING CLASS IMBALANCE\n",
            "======================================================================\n",
            "\n",
            "Original class distribution:\n",
            "  Class 0: 1781 (22.26%)\n",
            "  Class 1: 6219 (77.74%)\n",
            "\n",
            "Resampled class distribution:\n",
            "  Class 0: 6219 (50.00%)\n",
            "  Class 1: 6219 (50.00%)\n",
            "\n",
            "[STEP 6-8] Training Multiple ML Models...\n",
            "\n",
            "======================================================================\n",
            "MODEL TRAINING\n",
            "======================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "  ✓ Logistic Regression trained successfully\n",
            "    Accuracy: 0.6365\n",
            "    ROC-AUC: 0.6790\n",
            "\n",
            "Training Random Forest...\n",
            "  ✓ Random Forest trained successfully\n",
            "    Accuracy: 0.7685\n",
            "    ROC-AUC: 0.6689\n",
            "\n",
            "Training Gradient Boosting...\n",
            "  ✓ Gradient Boosting trained successfully\n",
            "    Accuracy: 0.7770\n",
            "    ROC-AUC: 0.6854\n",
            "\n",
            "Training SVM...\n",
            "  ✓ SVM trained successfully\n",
            "    Accuracy: 0.6715\n",
            "    ROC-AUC: 0.6480\n",
            "\n",
            "Training Neural Network...\n",
            "  ✓ Neural Network trained successfully\n",
            "    Accuracy: 0.6875\n",
            "    ROC-AUC: 0.5809\n",
            "✓ Model comparison saved\n",
            "\n",
            "======================================================================\n",
            "BEST MODEL: Gradient Boosting\n",
            "ROC-AUC: 0.6854\n",
            "======================================================================\n",
            "\n",
            "[STEP 7] Hyperparameter Tuning...\n",
            "\n",
            "[STEP 9] Applying Explainable AI (SHAP)...\n",
            "\n",
            "======================================================================\n",
            "MODEL EXPLAINABILITY (SHAP)\n",
            "======================================================================\n",
            "SHAP not available. Using feature importance from model instead.\n",
            "\n",
            "Top Features Influencing Churn:\n",
            "                feature  importance\n",
            "          contract_type    0.267148\n",
            "        support_tickets    0.170097\n",
            "             complaints    0.123150\n",
            "     payment_delay_days    0.058966\n",
            "        login_frequency    0.057876\n",
            "    payment_reliability    0.056058\n",
            "         payment_method    0.035896\n",
            "promotional_offers_used    0.027226\n",
            "          tenure_months    0.026843\n",
            "               location    0.022418\n",
            "  days_since_last_login    0.021661\n",
            "        tenure_category    0.021633\n",
            "          recency_score    0.016784\n",
            "      support_intensity    0.016079\n",
            "                 gender    0.015912\n",
            "\n",
            "[STEP 10] Customer Segmentation...\n",
            "\n",
            "======================================================================\n",
            "CUSTOMER SEGMENTATION\n",
            "======================================================================\n",
            "\n",
            "Segment Profiles:\n",
            "======================================================================\n",
            "         Avg Tenure  Avg Charges  Avg Logins  Avg Tickets  Churn Rate  Count  \\\n",
            "segment                                                                        \n",
            "0             13.62        68.98       15.13         1.99        0.76   4126   \n",
            "1             15.13        68.05       14.99         2.01        0.84   2854   \n",
            "2             56.42        66.50       14.87         2.07        0.69   1814   \n",
            "3             17.90        68.90       15.10         2.03        0.83   1206   \n",
            "\n",
            "        Segment Name  \n",
            "segment               \n",
            "0          High Risk  \n",
            "1          High Risk  \n",
            "2          High Risk  \n",
            "3          High Risk  \n",
            "\n",
            "✓ Customers segmented into 4 groups\n",
            "✓ Customer segmentation completed\n",
            "\n",
            "[STEP 11] Generating Churn Prevention Strategies...\n",
            "\n",
            "======================================================================\n",
            "CHURN PREVENTION STRATEGIES\n",
            "======================================================================\n",
            "\n",
            "Identified 2000 high-risk customers\n",
            "(Churn probability >= 84.85%)\n",
            "\n",
            "Sample Prevention Strategies:\n",
            "======================================================================\n",
            "\n",
            "Customer 1: CUST_000000\n",
            "  Churn Probability: 85.3%\n",
            "  Priority: High\n",
            "  Issues: High support tickets, No long-term commitment\n",
            "  Actions: Priority support escalation; Annual contract upgrade incentive\n",
            "\n",
            "Customer 2: CUST_000002\n",
            "  Churn Probability: 90.0%\n",
            "  Priority: High\n",
            "  Issues: No long-term commitment\n",
            "  Actions: Annual contract upgrade incentive\n",
            "\n",
            "Customer 3: CUST_000006\n",
            "  Churn Probability: 91.7%\n",
            "  Priority: High\n",
            "  Issues: High support tickets, No long-term commitment, High price point\n",
            "  Actions: Priority support escalation; Annual contract upgrade incentive; Customized package review\n",
            "✓ Prevention strategies generated\n",
            "\n",
            "[STEP 12] Saving Models and Pipelines...\n",
            "\n",
            "✓ Pipeline saved to outputs/churn_prediction_pipeline.pkl\n",
            "✓ All models saved\n",
            "\n",
            "[STEP 13] Testing Inference Pipeline...\n",
            "✓ Pipeline loaded from outputs/churn_prediction_pipeline.pkl\n",
            "\n",
            "======================================================================\n",
            "DATA CLEANING\n",
            "======================================================================\n",
            "\n",
            "Missing Values Before Cleaning:\n",
            "age                    1\n",
            "feature_usage_score    2\n",
            "dtype: int64\n",
            "\n",
            "monthly_charges: Capped 0 outliers\n",
            "\n",
            "total_charges: Capped 0 outliers\n",
            "\n",
            "download_volume_gb: Capped 1 outliers\n",
            "\n",
            "Missing Values After Cleaning:\n",
            "0 total missing values\n",
            "\n",
            "======================================================================\n",
            "FEATURE ENGINEERING\n",
            "======================================================================\n",
            "\n",
            "Created 32 features\n",
            "New engineered features: 13\n",
            "\n",
            "Sample Predictions on New Customers:\n",
            "customer_id  tenure_months  monthly_charges  churn_probability risk_level\n",
            "CUST_006252              9        95.285992           0.826474       High\n",
            "CUST_004684             44       107.509282           0.706540       High\n",
            "CUST_001731              9        97.444195           0.677389       High\n",
            "CUST_004742             15        96.296831           0.807099       High\n",
            "CUST_004521             25        61.493273           0.608440       High\n",
            "CUST_006340             61       129.020061           0.896169       High\n",
            "CUST_000576              8        81.436698           0.825515       High\n",
            "CUST_005202             50       105.907927           0.901365       High\n",
            "CUST_006363             15        49.661277           0.681339       High\n",
            "CUST_000439             43        65.254851           0.680830       High\n",
            "\n",
            "[STEP 14] Generating Business Insights Report...\n",
            "\n",
            "======================================================================\n",
            "SYSTEM EXECUTION COMPLETED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "Outputs Generated:\n",
            "  1. raw_customer_data.csv - Original dataset\n",
            "  2. processed_customer_data.csv - Cleaned & engineered features\n",
            "  3. statistical_summary.csv - Comprehensive EDA summary\n",
            "  4. eda_visualizations.png - Exploratory visualizations\n",
            "  5. model_comparison.png - Model performance comparison\n",
            "  6. shap_feature_importance.png - Feature importance analysis\n",
            "  7. feature_importance.csv - Top churn predictors\n",
            "  8. customer_segments.png - Segmentation visualizations\n",
            "  9. segmented_customers.csv - Customers with segments\n",
            "  10. churn_prevention_strategies.csv - Actionable strategies\n",
            "  11. churn_prediction_pipeline.pkl - Production-ready pipeline\n",
            "  12. all_trained_models.pkl - All trained models\n",
            "  13. sample_predictions.csv - Inference demo results\n",
            "  14. business_insights_report.json - Executive summary\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Imbalanced Learning - Manual implementation\n",
        "XGBOOST_AVAILABLE = False\n",
        "SHAP_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "class SimpleSMOTE:\n",
        "    \"\"\"Simple SMOTE implementation without external dependencies.\"\"\"\n",
        "\n",
        "    def __init__(self, k_neighbors=5, random_state=42):\n",
        "        self.k_neighbors = k_neighbors\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit_resample(self, X, y):\n",
        "        \"\"\"Oversample minority class using SMOTE.\"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "        # Find minority and majority classes\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        minority_class = unique[np.argmin(counts)]\n",
        "        majority_class = unique[np.argmax(counts)]\n",
        "\n",
        "        minority_samples = X[y == minority_class]\n",
        "        minority_labels = y[y == minority_class]\n",
        "        majority_samples = X[y == majority_class]\n",
        "        majority_labels = y[y == majority_class]\n",
        "\n",
        "        # Number of synthetic samples to generate\n",
        "        n_synthetic = len(majority_samples) - len(minority_samples)\n",
        "\n",
        "        if n_synthetic <= 0:\n",
        "            return X, y\n",
        "\n",
        "        # Generate synthetic samples\n",
        "        synthetic_samples = []\n",
        "\n",
        "        for _ in range(n_synthetic):\n",
        "            # Random sample from minority class\n",
        "            idx = np.random.randint(0, len(minority_samples))\n",
        "            sample = minority_samples[idx]\n",
        "\n",
        "            # Find k nearest neighbors\n",
        "            distances = np.sum((minority_samples - sample) ** 2, axis=1)\n",
        "            k_nearest_idx = np.argsort(distances)[1:self.k_neighbors+1]\n",
        "\n",
        "            # Choose random neighbor\n",
        "            neighbor_idx = np.random.choice(k_nearest_idx)\n",
        "            neighbor = minority_samples[neighbor_idx]\n",
        "\n",
        "            # Generate synthetic sample\n",
        "            alpha = np.random.random()\n",
        "            synthetic_sample = sample + alpha * (neighbor - sample)\n",
        "            synthetic_samples.append(synthetic_sample)\n",
        "\n",
        "        # Combine all samples\n",
        "        X_resampled = np.vstack([\n",
        "            majority_samples,\n",
        "            minority_samples,\n",
        "            np.array(synthetic_samples)\n",
        "        ])\n",
        "\n",
        "        y_resampled = np.hstack([\n",
        "            majority_labels,\n",
        "            minority_labels,\n",
        "            np.full(n_synthetic, minority_class)\n",
        "        ])\n",
        "\n",
        "        # Shuffle\n",
        "        shuffle_idx = np.random.permutation(len(X_resampled))\n",
        "        X_resampled = X_resampled[shuffle_idx]\n",
        "        y_resampled = y_resampled[shuffle_idx]\n",
        "\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "\n",
        "class CustomerDataGenerator:\n",
        "    \"\"\"Generate synthetic but realistic customer data for churn prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, n_samples=10000, random_state=42):\n",
        "        self.n_samples = n_samples\n",
        "        self.random_state = random_state\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    def generate_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate comprehensive customer dataset.\"\"\"\n",
        "\n",
        "        # Demographics\n",
        "        customer_ids = [f\"CUST_{i:06d}\" for i in range(self.n_samples)]\n",
        "        age = np.random.normal(35, 12, self.n_samples).clip(18, 75).astype(int)\n",
        "        gender = np.random.choice(['M', 'F', 'Other'], self.n_samples, p=[0.48, 0.48, 0.04])\n",
        "        location = np.random.choice(['Urban', 'Suburban', 'Rural'], self.n_samples, p=[0.5, 0.35, 0.15])\n",
        "\n",
        "        # Service Information\n",
        "        tenure_months = np.random.exponential(24, self.n_samples).clip(0, 72).astype(int)\n",
        "        contract_type = np.random.choice(['Month-to-Month', '1-Year', '2-Year'], self.n_samples, p=[0.5, 0.3, 0.2])\n",
        "        monthly_charges = np.random.normal(65, 30, self.n_samples).clip(20, 150)\n",
        "\n",
        "        # Usage Behavior\n",
        "        login_frequency = np.random.poisson(15, self.n_samples).clip(0, 100)\n",
        "        feature_usage_score = np.random.beta(2, 5, self.n_samples) * 100\n",
        "        avg_session_duration = np.random.gamma(2, 15, self.n_samples).clip(5, 180)\n",
        "\n",
        "        # Engagement Metrics\n",
        "        support_tickets = np.random.poisson(2, self.n_samples)\n",
        "        complaints = np.random.poisson(0.5, self.n_samples)\n",
        "        days_since_last_login = np.random.exponential(5, self.n_samples).clip(0, 60).astype(int)\n",
        "\n",
        "        # Billing Information\n",
        "        payment_delay_days = np.random.exponential(2, self.n_samples).clip(0, 30).astype(int)\n",
        "        payment_method = np.random.choice(['Credit Card', 'Bank Transfer', 'Digital Wallet'],\n",
        "                                         self.n_samples, p=[0.5, 0.3, 0.2])\n",
        "        total_charges = monthly_charges * tenure_months + np.random.normal(0, 50, self.n_samples)\n",
        "\n",
        "        # Additional Features\n",
        "        num_referrals = np.random.poisson(1, self.n_samples)\n",
        "        promotional_offers_used = np.random.poisson(2, self.n_samples)\n",
        "        download_volume_gb = np.random.lognormal(3, 1.5, self.n_samples).clip(0, 500)\n",
        "\n",
        "        # Churn Logic (Creating realistic churn patterns)\n",
        "        churn_probability = self._calculate_churn_probability(\n",
        "            tenure_months, contract_type, support_tickets, complaints,\n",
        "            payment_delay_days, login_frequency, days_since_last_login,\n",
        "            monthly_charges, feature_usage_score\n",
        "        )\n",
        "\n",
        "        churn = (np.random.random(self.n_samples) < churn_probability).astype(int)\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame({\n",
        "            'customer_id': customer_ids,\n",
        "            'age': age,\n",
        "            'gender': gender,\n",
        "            'location': location,\n",
        "            'tenure_months': tenure_months,\n",
        "            'contract_type': contract_type,\n",
        "            'monthly_charges': monthly_charges,\n",
        "            'total_charges': total_charges,\n",
        "            'login_frequency': login_frequency,\n",
        "            'feature_usage_score': feature_usage_score,\n",
        "            'avg_session_duration': avg_session_duration,\n",
        "            'support_tickets': support_tickets,\n",
        "            'complaints': complaints,\n",
        "            'days_since_last_login': days_since_last_login,\n",
        "            'payment_delay_days': payment_delay_days,\n",
        "            'payment_method': payment_method,\n",
        "            'num_referrals': num_referrals,\n",
        "            'promotional_offers_used': promotional_offers_used,\n",
        "            'download_volume_gb': download_volume_gb,\n",
        "            'churn': churn\n",
        "        })\n",
        "\n",
        "        # Introduce missing values and outliers\n",
        "        df = self._introduce_data_issues(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _calculate_churn_probability(self, tenure, contract, tickets, complaints,\n",
        "                                    payment_delay, login_freq, last_login,\n",
        "                                    charges, feature_usage):\n",
        "        \"\"\"Calculate realistic churn probability based on customer attributes.\"\"\"\n",
        "\n",
        "        prob = np.zeros(len(tenure))\n",
        "\n",
        "        # Tenure effect (newer customers more likely to churn)\n",
        "        prob += 0.3 * np.exp(-tenure / 12)\n",
        "\n",
        "        # Contract type effect\n",
        "        contract_risk = np.where(contract == 'Month-to-Month', 0.25,\n",
        "                        np.where(contract == '1-Year', 0.1, 0.05))\n",
        "        prob += contract_risk\n",
        "\n",
        "        # Support issues\n",
        "        prob += 0.05 * tickets + 0.1 * complaints\n",
        "\n",
        "        # Payment issues\n",
        "        prob += 0.02 * payment_delay\n",
        "\n",
        "        # Engagement (low engagement = high churn)\n",
        "        prob += 0.01 * (30 - login_freq).clip(0, 30)\n",
        "        prob += 0.01 * last_login\n",
        "\n",
        "        # Feature usage (low usage = high churn)\n",
        "        prob += 0.002 * (100 - feature_usage)\n",
        "\n",
        "        # Price sensitivity\n",
        "        prob += 0.001 * (charges - 50).clip(0, 100)\n",
        "\n",
        "        return prob.clip(0, 0.9)\n",
        "\n",
        "    def _introduce_data_issues(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Introduce realistic data quality issues.\"\"\"\n",
        "\n",
        "        # Missing values (5-10% for some columns)\n",
        "        for col in ['age', 'feature_usage_score', 'total_charges', 'download_volume_gb']:\n",
        "            mask = np.random.random(len(df)) < 0.07\n",
        "            df.loc[mask, col] = np.nan\n",
        "\n",
        "        # Outliers\n",
        "        outlier_mask = np.random.random(len(df)) < 0.02\n",
        "        df.loc[outlier_mask, 'monthly_charges'] *= np.random.uniform(2, 5, outlier_mask.sum())\n",
        "\n",
        "        # Inconsistent entries\n",
        "        inconsistent_mask = np.random.random(len(df)) < 0.01\n",
        "        df.loc[inconsistent_mask, 'gender'] = np.random.choice(['M', 'F', 'Other', 'Unknown', 'male', 'female'],\n",
        "                                                                inconsistent_mask.sum())\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Handle data cleaning and feature engineering.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.feature_names = None\n",
        "\n",
        "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Comprehensive data cleaning.\"\"\"\n",
        "\n",
        "        df = df.copy()\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DATA CLEANING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Fix inconsistent gender values\n",
        "        gender_mapping = {'male': 'M', 'female': 'F', 'Unknown': 'Other'}\n",
        "        df['gender'] = df['gender'].replace(gender_mapping)\n",
        "\n",
        "        # Handle missing values\n",
        "        print(\"\\nMissing Values Before Cleaning:\")\n",
        "        print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "\n",
        "        # Numerical columns - fill with median\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        for col in num_cols:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        # Categorical columns - fill with mode\n",
        "        cat_cols = df.select_dtypes(include=['object']).columns\n",
        "        cat_cols = cat_cols.drop('customer_id')\n",
        "        for col in cat_cols:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "        # Handle outliers using IQR method\n",
        "        for col in ['monthly_charges', 'total_charges', 'download_volume_gb']:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 3 * IQR\n",
        "            upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "            outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "            df[col] = df[col].clip(lower_bound, upper_bound)\n",
        "            print(f\"\\n{col}: Capped {outliers_before} outliers\")\n",
        "\n",
        "        print(\"\\nMissing Values After Cleaning:\")\n",
        "        print(df.isnull().sum().sum(), \"total missing values\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create advanced engineered features.\"\"\"\n",
        "\n",
        "        df = df.copy()\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FEATURE ENGINEERING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Customer Lifetime Value (CLV)\n",
        "        df['customer_lifetime_value'] = df['monthly_charges'] * df['tenure_months']\n",
        "\n",
        "        # Average Revenue Per Month\n",
        "        df['avg_revenue_per_month'] = df['total_charges'] / (df['tenure_months'] + 1)\n",
        "\n",
        "        # Engagement Score (composite metric)\n",
        "        df['engagement_score'] = (\n",
        "            (df['login_frequency'] / df['login_frequency'].max()) * 0.4 +\n",
        "            (df['feature_usage_score'] / 100) * 0.3 +\n",
        "            (df['avg_session_duration'] / df['avg_session_duration'].max()) * 0.3\n",
        "        ) * 100\n",
        "\n",
        "        # Engagement Decay Rate\n",
        "        df['engagement_decay'] = df['days_since_last_login'] / (df['tenure_months'] + 1)\n",
        "\n",
        "        # Support Issue Intensity\n",
        "        df['support_intensity'] = (df['support_tickets'] + 2 * df['complaints']) / (df['tenure_months'] + 1)\n",
        "\n",
        "        # Payment Reliability Score\n",
        "        df['payment_reliability'] = 100 - (df['payment_delay_days'] * 2).clip(0, 100)\n",
        "\n",
        "        # Value-to-Cost Ratio\n",
        "        df['value_cost_ratio'] = df['feature_usage_score'] / (df['monthly_charges'] + 1)\n",
        "\n",
        "        # Tenure Categories\n",
        "        df['tenure_category'] = pd.cut(df['tenure_months'],\n",
        "                                       bins=[0, 6, 12, 24, 72],\n",
        "                                       labels=['New', 'Growing', 'Established', 'Loyal'])\n",
        "\n",
        "        # High-Risk Indicators\n",
        "        df['is_high_complaints'] = (df['complaints'] > df['complaints'].quantile(0.75)).astype(int)\n",
        "        df['is_payment_delayed'] = (df['payment_delay_days'] > 5).astype(int)\n",
        "        df['is_low_engagement'] = (df['engagement_score'] < df['engagement_score'].quantile(0.25)).astype(int)\n",
        "\n",
        "        # Referral Activity\n",
        "        df['is_active_referrer'] = (df['num_referrals'] > 0).astype(int)\n",
        "\n",
        "        # Usage Intensity\n",
        "        df['usage_per_dollar'] = df['download_volume_gb'] / (df['monthly_charges'] + 1)\n",
        "\n",
        "        # Recency Score\n",
        "        df['recency_score'] = 100 - (df['days_since_last_login'] * 2).clip(0, 100)\n",
        "\n",
        "        print(f\"\\nCreated {len([c for c in df.columns if c not in ['customer_id', 'churn']])} features\")\n",
        "        print(f\"New engineered features: {len(df.columns) - 20}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def encode_categorical(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"Encode categorical variables.\"\"\"\n",
        "\n",
        "        df = df.copy()\n",
        "        categorical_cols = ['gender', 'location', 'contract_type', 'payment_method', 'tenure_category']\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                if fit:\n",
        "                    le = LabelEncoder()\n",
        "                    df[col] = le.fit_transform(df[col].astype(str))\n",
        "                    self.label_encoders[col] = le\n",
        "                else:\n",
        "                    if col in self.label_encoders:\n",
        "                        # Handle unseen labels\n",
        "                        le = self.label_encoders[col]\n",
        "                        df[col] = df[col].astype(str).apply(\n",
        "                            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
        "                        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_features(self, df: pd.DataFrame, fit: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Prepare features for modeling.\"\"\"\n",
        "\n",
        "        df = df.copy()\n",
        "\n",
        "        # Separate features and target\n",
        "        feature_cols = [col for col in df.columns if col not in ['customer_id', 'churn']]\n",
        "        X = df[feature_cols].values\n",
        "        y = df['churn'].values if 'churn' in df.columns else None\n",
        "\n",
        "        if fit:\n",
        "            self.feature_names = feature_cols\n",
        "            X = self.scaler.fit_transform(X)\n",
        "        else:\n",
        "            X = self.scaler.transform(X)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "class ExploratoryAnalysis:\n",
        "    \"\"\"Perform comprehensive EDA.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_statistical_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Generate detailed statistical summary.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STATISTICAL SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        summary = df.describe(include='all').T\n",
        "        summary['missing'] = df.isnull().sum()\n",
        "        summary['missing_pct'] = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "        print(\"\\nDataset Shape:\", df.shape)\n",
        "        print(\"\\nChurn Distribution:\")\n",
        "        print(df['churn'].value_counts())\n",
        "        print(f\"\\nChurn Rate: {df['churn'].mean() * 100:.2f}%\")\n",
        "\n",
        "        return summary\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_churn_analysis(df: pd.DataFrame, save_path: str = None):\n",
        "        \"\"\"Create comprehensive churn visualizations.\"\"\"\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "        # 1. Churn Distribution\n",
        "        ax1 = plt.subplot(3, 4, 1)\n",
        "        churn_counts = df['churn'].value_counts()\n",
        "        ax1.pie(churn_counts, labels=['Retained', 'Churned'], autopct='%1.1f%%', startangle=90)\n",
        "        ax1.set_title('Churn Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "        # 2. Churn by Contract Type\n",
        "        ax2 = plt.subplot(3, 4, 2)\n",
        "        pd.crosstab(df['contract_type'], df['churn'], normalize='index').plot(kind='bar', ax=ax2)\n",
        "        ax2.set_title('Churn Rate by Contract Type', fontsize=10)\n",
        "        ax2.set_xlabel('Contract Type')\n",
        "        ax2.set_ylabel('Proportion')\n",
        "        ax2.legend(['Retained', 'Churned'])\n",
        "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "        # 3. Tenure Distribution\n",
        "        ax3 = plt.subplot(3, 4, 3)\n",
        "        df[df['churn']==0]['tenure_months'].hist(bins=30, alpha=0.5, label='Retained', ax=ax3)\n",
        "        df[df['churn']==1]['tenure_months'].hist(bins=30, alpha=0.5, label='Churned', ax=ax3)\n",
        "        ax3.set_title('Tenure Distribution', fontsize=10)\n",
        "        ax3.set_xlabel('Tenure (months)')\n",
        "        ax3.legend()\n",
        "\n",
        "        # 4. Monthly Charges\n",
        "        ax4 = plt.subplot(3, 4, 4)\n",
        "        df[df['churn']==0]['monthly_charges'].hist(bins=30, alpha=0.5, label='Retained', ax=ax4)\n",
        "        df[df['churn']==1]['monthly_charges'].hist(bins=30, alpha=0.5, label='Churned', ax=ax4)\n",
        "        ax4.set_title('Monthly Charges Distribution', fontsize=10)\n",
        "        ax4.set_xlabel('Monthly Charges ($)')\n",
        "        ax4.legend()\n",
        "\n",
        "        # 5. Support Tickets\n",
        "        ax5 = plt.subplot(3, 4, 5)\n",
        "        df.groupby('support_tickets')['churn'].mean().plot(kind='bar', ax=ax5)\n",
        "        ax5.set_title('Churn Rate by Support Tickets', fontsize=10)\n",
        "        ax5.set_xlabel('Number of Support Tickets')\n",
        "        ax5.set_ylabel('Churn Rate')\n",
        "\n",
        "        # 6. Payment Delay\n",
        "        ax6 = plt.subplot(3, 4, 6)\n",
        "        df.boxplot(column='payment_delay_days', by='churn', ax=ax6)\n",
        "        ax6.set_title('Payment Delay by Churn', fontsize=10)\n",
        "        ax6.set_xlabel('Churn Status')\n",
        "        ax6.set_ylabel('Payment Delay (days)')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 7. Login Frequency\n",
        "        ax7 = plt.subplot(3, 4, 7)\n",
        "        df.boxplot(column='login_frequency', by='churn', ax=ax7)\n",
        "        ax7.set_title('Login Frequency by Churn', fontsize=10)\n",
        "        ax7.set_xlabel('Churn Status')\n",
        "        ax7.set_ylabel('Login Frequency')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 8. Feature Usage Score\n",
        "        ax8 = plt.subplot(3, 4, 8)\n",
        "        df.boxplot(column='feature_usage_score', by='churn', ax=ax8)\n",
        "        ax8.set_title('Feature Usage by Churn', fontsize=10)\n",
        "        ax8.set_xlabel('Churn Status')\n",
        "        ax8.set_ylabel('Feature Usage Score')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 9. Correlation Heatmap (top features)\n",
        "        ax9 = plt.subplot(3, 4, 9)\n",
        "        numeric_cols = ['tenure_months', 'monthly_charges', 'login_frequency',\n",
        "                       'support_tickets', 'complaints', 'payment_delay_days', 'churn']\n",
        "        corr_matrix = df[numeric_cols].corr()\n",
        "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', ax=ax9, cbar_kws={'shrink': 0.8})\n",
        "        ax9.set_title('Feature Correlation Matrix', fontsize=10)\n",
        "\n",
        "        # 10. Churn by Location\n",
        "        ax10 = plt.subplot(3, 4, 10)\n",
        "        pd.crosstab(df['location'], df['churn'], normalize='index').plot(kind='bar', ax=ax10)\n",
        "        ax10.set_title('Churn Rate by Location', fontsize=10)\n",
        "        ax10.set_xlabel('Location')\n",
        "        ax10.legend(['Retained', 'Churned'])\n",
        "        plt.setp(ax10.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "        # 11. Days Since Last Login\n",
        "        ax11 = plt.subplot(3, 4, 11)\n",
        "        df[df['churn']==0]['days_since_last_login'].hist(bins=30, alpha=0.5, label='Retained', ax=ax11)\n",
        "        df[df['churn']==1]['days_since_last_login'].hist(bins=30, alpha=0.5, label='Churned', ax=ax11)\n",
        "        ax11.set_title('Days Since Last Login', fontsize=10)\n",
        "        ax11.set_xlabel('Days')\n",
        "        ax11.legend()\n",
        "\n",
        "        # 12. Complaints Distribution\n",
        "        ax12 = plt.subplot(3, 4, 12)\n",
        "        df.groupby('complaints')['churn'].mean().plot(kind='bar', ax=ax12, color='coral')\n",
        "        ax12.set_title('Churn Rate by Complaints', fontsize=10)\n",
        "        ax12.set_xlabel('Number of Complaints')\n",
        "        ax12.set_ylabel('Churn Rate')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        return fig\n",
        "\n",
        "\n",
        "class ChurnPredictionModels:\n",
        "    \"\"\"Train and evaluate multiple ML models.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.best_model = None\n",
        "        self.best_model_name = None\n",
        "\n",
        "    def handle_class_imbalance(self, X_train, y_train, method='smote'):\n",
        "        \"\"\"Handle class imbalance using SMOTE or other techniques.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"HANDLING CLASS IMBALANCE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"\\nOriginal class distribution:\")\n",
        "        unique, counts = np.unique(y_train, return_counts=True)\n",
        "        for u, c in zip(unique, counts):\n",
        "            print(f\"  Class {u}: {c} ({c/len(y_train)*100:.2f}%)\")\n",
        "\n",
        "        if method == 'smote':\n",
        "            smote = SimpleSMOTE(random_state=42)\n",
        "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "        else:\n",
        "            X_resampled, y_resampled = X_train, y_train\n",
        "\n",
        "        print(f\"\\nResampled class distribution:\")\n",
        "        unique, counts = np.unique(y_resampled, return_counts=True)\n",
        "        for u, c in zip(unique, counts):\n",
        "            print(f\"  Class {u}: {c} ({c/len(y_resampled)*100:.2f}%)\")\n",
        "\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "    def train_models(self, X_train, y_train, X_test, y_test):\n",
        "        \"\"\"Train multiple ML models.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"MODEL TRAINING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Define models\n",
        "        models_dict = {\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "            'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Train each model\n",
        "        for name, model in models_dict.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "\n",
        "            try:\n",
        "                model.fit(X_train, y_train)\n",
        "                self.models[name] = model\n",
        "\n",
        "                # Predictions\n",
        "                y_pred = model.predict(X_test)\n",
        "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "                # Evaluate\n",
        "                self.results[name] = self._evaluate_model(y_test, y_pred, y_pred_proba, name)\n",
        "\n",
        "                print(f\"  ✓ {name} trained successfully\")\n",
        "                print(f\"    Accuracy: {self.results[name]['accuracy']:.4f}\")\n",
        "                print(f\"    ROC-AUC: {self.results[name]['roc_auc']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Error training {name}: {str(e)}\")\n",
        "\n",
        "    def _evaluate_model(self, y_true, y_pred, y_pred_proba, model_name):\n",
        "        \"\"\"Comprehensive model evaluation.\"\"\"\n",
        "\n",
        "        results = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
        "            'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def hyperparameter_tuning(self, X_train, y_train, model_name='Random Forest'):\n",
        "        \"\"\"Perform hyperparameter tuning using GridSearch.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"HYPERPARAMETER TUNING - {model_name}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        if model_name == 'Random Forest':\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200],\n",
        "                'max_depth': [10, 20, 30, None],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4]\n",
        "            }\n",
        "            base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "        else:\n",
        "            print(f\"Hyperparameter tuning not configured for {model_name}\")\n",
        "            return None\n",
        "\n",
        "        # GridSearchCV\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=base_model,\n",
        "            param_grid=param_grid,\n",
        "            cv=3,\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"Starting grid search...\")\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "        print(f\"Best ROC-AUC score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "    def plot_model_comparison(self, save_path=None):\n",
        "        \"\"\"Visualize model performance comparison.\"\"\"\n",
        "\n",
        "        fig = plt.figure(figsize=(18, 10))\n",
        "\n",
        "        # Prepare data\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
        "        model_names = list(self.results.keys())\n",
        "\n",
        "        # 1. Metrics Comparison\n",
        "        ax1 = plt.subplot(2, 3, 1)\n",
        "        metric_data = {metric: [self.results[model][metric] for model in model_names] for metric in metrics}\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.15\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            ax1.bar(x + i*width, metric_data[metric], width, label=metric.replace('_', ' ').title())\n",
        "\n",
        "        ax1.set_xlabel('Models')\n",
        "        ax1.set_ylabel('Score')\n",
        "        ax1.set_title('Model Performance Comparison', fontweight='bold')\n",
        "        ax1.set_xticks(x + width * 2)\n",
        "        ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "        ax1.legend(loc='lower right')\n",
        "        ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # 2-4. Confusion Matrices for top 3 models\n",
        "        sorted_models = sorted(self.results.items(), key=lambda x: x[1]['roc_auc'], reverse=True)\n",
        "\n",
        "        for idx, (model_name, results) in enumerate(sorted_models[:3], start=2):\n",
        "            ax = plt.subplot(2, 3, idx)\n",
        "            cm = results['confusion_matrix']\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
        "            ax.set_title(f'{model_name}\\nAccuracy: {results[\"accuracy\"]:.3f}', fontsize=10)\n",
        "            ax.set_xlabel('Predicted')\n",
        "            ax.set_ylabel('Actual')\n",
        "\n",
        "        # 5. ROC Curves\n",
        "        ax5 = plt.subplot(2, 3, 5)\n",
        "        for model_name, results in self.results.items():\n",
        "            y_test = results.get('y_true', None)\n",
        "            if y_test is None:\n",
        "                continue\n",
        "            fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])\n",
        "            ax5.plot(fpr, tpr, label=f\"{model_name} (AUC={results['roc_auc']:.3f})\")\n",
        "\n",
        "        ax5.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "        ax5.set_xlabel('False Positive Rate')\n",
        "        ax5.set_ylabel('True Positive Rate')\n",
        "        ax5.set_title('ROC Curves Comparison', fontweight='bold')\n",
        "        ax5.legend(loc='lower right')\n",
        "        ax5.grid(alpha=0.3)\n",
        "\n",
        "        # 6. Precision-Recall Curves\n",
        "        ax6 = plt.subplot(2, 3, 6)\n",
        "        for model_name, results in self.results.items():\n",
        "            y_test = results.get('y_true', None)\n",
        "            if y_test is None:\n",
        "                continue\n",
        "            precision, recall, _ = precision_recall_curve(y_test, results['y_pred_proba'])\n",
        "            ax6.plot(recall, precision, label=f\"{model_name}\")\n",
        "\n",
        "        ax6.set_xlabel('Recall')\n",
        "        ax6.set_ylabel('Precision')\n",
        "        ax6.set_title('Precision-Recall Curves', fontweight='bold')\n",
        "        ax6.legend(loc='lower left')\n",
        "        ax6.grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def select_best_model(self):\n",
        "        \"\"\"Select the best performing model.\"\"\"\n",
        "\n",
        "        best_auc = 0\n",
        "        best_name = None\n",
        "\n",
        "        for name, results in self.results.items():\n",
        "            if results['roc_auc'] > best_auc:\n",
        "                best_auc = results['roc_auc']\n",
        "                best_name = name\n",
        "\n",
        "        self.best_model = self.models[best_name]\n",
        "        self.best_model_name = best_name\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"BEST MODEL: {best_name}\")\n",
        "        print(f\"ROC-AUC: {best_auc:.4f}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        return best_name, self.best_model\n",
        "\n",
        "\n",
        "class ModelExplainability:\n",
        "    \"\"\"Apply SHAP for model interpretability.\"\"\"\n",
        "\n",
        "    def __init__(self, model, X_train, feature_names):\n",
        "        self.model = model\n",
        "        self.X_train = X_train\n",
        "        self.feature_names = feature_names\n",
        "        self.explainer = None\n",
        "        self.shap_values = None\n",
        "\n",
        "    def compute_shap_values(self, X_test, sample_size=100):\n",
        "        \"\"\"Compute SHAP values for model predictions.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"MODEL EXPLAINABILITY (SHAP)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        if not SHAP_AVAILABLE:\n",
        "            print(\"SHAP not available. Using feature importance from model instead.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Use TreeExplainer for tree-based models\n",
        "            if hasattr(self.model, 'tree_') or 'Forest' in str(type(self.model)) or 'XGB' in str(type(self.model)):\n",
        "                import shap\n",
        "                self.explainer = shap.TreeExplainer(self.model)\n",
        "            else:\n",
        "                # Use KernelExplainer for other models\n",
        "                import shap\n",
        "                background = shap.kmeans(self.X_train, 10)\n",
        "                self.explainer = shap.KernelExplainer(self.model.predict_proba, background)\n",
        "\n",
        "            # Compute SHAP values on sample\n",
        "            X_sample = X_test[:sample_size] if len(X_test) > sample_size else X_test\n",
        "            self.shap_values = self.explainer.shap_values(X_sample)\n",
        "\n",
        "            # For binary classification, get positive class SHAP values\n",
        "            if isinstance(self.shap_values, list):\n",
        "                self.shap_values = self.shap_values[1]\n",
        "\n",
        "            print(f\"✓ SHAP values computed for {len(X_sample)} samples\")\n",
        "\n",
        "            return self.shap_values\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error computing SHAP values: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_feature_importance(self, save_path=None):\n",
        "        \"\"\"Plot feature importance.\"\"\"\n",
        "\n",
        "        if self.shap_values is None and not SHAP_AVAILABLE:\n",
        "            # Use model's feature importance instead\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                importances = self.model.feature_importances_\n",
        "            elif hasattr(self.model, 'coef_'):\n",
        "                importances = np.abs(self.model.coef_[0])\n",
        "            else:\n",
        "                print(\"Feature importance not available.\")\n",
        "                return None\n",
        "\n",
        "            # Create bar plot\n",
        "            fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "            # Sort by importance\n",
        "            indices = np.argsort(importances)[-20:]  # Top 20\n",
        "\n",
        "            ax.barh(range(len(indices)), importances[indices])\n",
        "            ax.set_yticks(range(len(indices)))\n",
        "            ax.set_yticklabels([self.feature_names[i] for i in indices])\n",
        "            ax.set_xlabel('Importance')\n",
        "            ax.set_title('Top Features Influencing Churn (Model Feature Importance)',\n",
        "                        fontweight='bold', fontsize=14)\n",
        "            ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            if save_path:\n",
        "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "            return fig\n",
        "\n",
        "        if self.shap_values is None:\n",
        "            print(\"SHAP values not computed yet.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            import shap\n",
        "            fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "            # Summary plot\n",
        "            shap.summary_plot(\n",
        "                self.shap_values,\n",
        "                features=self.X_train[:100] if len(self.X_train) > 100 else self.X_train,\n",
        "                feature_names=self.feature_names,\n",
        "                plot_type='bar',\n",
        "                show=False\n",
        "            )\n",
        "            plt.title('Top Features Influencing Churn (SHAP)', fontweight='bold', fontsize=14)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            if save_path:\n",
        "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting SHAP: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_feature_importance_scores(self, top_n=15):\n",
        "        \"\"\"Get top N most important features.\"\"\"\n",
        "\n",
        "        if self.shap_values is None:\n",
        "            # Fallback to model feature importance\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                importances = self.model.feature_importances_\n",
        "            elif hasattr(self.model, 'coef_'):\n",
        "                importances = np.abs(self.model.coef_[0])\n",
        "            else:\n",
        "                print(\"Feature importance not available for this model.\")\n",
        "                return None\n",
        "        else:\n",
        "            # Use SHAP values\n",
        "            importances = np.abs(self.shap_values).mean(axis=0)\n",
        "\n",
        "        # Create DataFrame\n",
        "        feature_imp_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False).head(top_n)\n",
        "\n",
        "        print(\"\\nTop Features Influencing Churn:\")\n",
        "        print(feature_imp_df.to_string(index=False))\n",
        "\n",
        "        return feature_imp_df\n",
        "\n",
        "\n",
        "class CustomerSegmentation:\n",
        "    \"\"\"Cluster customers into behavioral segments.\"\"\"\n",
        "\n",
        "    def __init__(self, n_clusters=4):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.kmeans = None\n",
        "        self.segment_profiles = None\n",
        "\n",
        "    def perform_segmentation(self, df: pd.DataFrame, feature_cols: List[str]):\n",
        "        \"\"\"Perform K-Means clustering.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CUSTOMER SEGMENTATION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Select features for clustering\n",
        "        X_cluster = df[feature_cols].values\n",
        "\n",
        "        # Standardize\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X_cluster)\n",
        "\n",
        "        # K-Means\n",
        "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
        "        clusters = self.kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        df['segment'] = clusters\n",
        "\n",
        "        # Analyze segments\n",
        "        self._analyze_segments(df)\n",
        "\n",
        "        print(f\"\\n✓ Customers segmented into {self.n_clusters} groups\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _analyze_segments(self, df: pd.DataFrame):\n",
        "        \"\"\"Analyze and profile customer segments.\"\"\"\n",
        "\n",
        "        print(\"\\nSegment Profiles:\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        segment_summary = df.groupby('segment').agg({\n",
        "            'tenure_months': 'mean',\n",
        "            'monthly_charges': 'mean',\n",
        "            'login_frequency': 'mean',\n",
        "            'support_tickets': 'mean',\n",
        "            'churn': 'mean',\n",
        "            'customer_id': 'count'\n",
        "        }).round(2)\n",
        "\n",
        "        segment_summary.columns = ['Avg Tenure', 'Avg Charges', 'Avg Logins',\n",
        "                                   'Avg Tickets', 'Churn Rate', 'Count']\n",
        "\n",
        "        # Name segments\n",
        "        segment_names = []\n",
        "        for idx, row in segment_summary.iterrows():\n",
        "            if row['Churn Rate'] > 0.5:\n",
        "                name = \"High Risk\"\n",
        "            elif row['Avg Tenure'] > 24 and row['Churn Rate'] < 0.2:\n",
        "                name = \"Loyal Customers\"\n",
        "            elif row['Avg Logins'] < 10:\n",
        "                name = \"Low Engagement\"\n",
        "            else:\n",
        "                name = \"Standard\"\n",
        "            segment_names.append(name)\n",
        "\n",
        "        segment_summary['Segment Name'] = segment_names\n",
        "\n",
        "        print(segment_summary)\n",
        "\n",
        "        self.segment_profiles = segment_summary\n",
        "\n",
        "    def visualize_segments(self, df: pd.DataFrame, save_path=None):\n",
        "        \"\"\"Visualize customer segments.\"\"\"\n",
        "\n",
        "        fig = plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # PCA for 2D visualization\n",
        "        feature_cols = ['tenure_months', 'monthly_charges', 'login_frequency',\n",
        "                       'support_tickets', 'complaints', 'payment_delay_days']\n",
        "        X = df[feature_cols].values\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        X_pca = pca.fit_transform(StandardScaler().fit_transform(X))\n",
        "\n",
        "        # 1. Segment Distribution\n",
        "        ax1 = plt.subplot(2, 3, 1)\n",
        "        df['segment'].value_counts().plot(kind='bar', ax=ax1, color='skyblue')\n",
        "        ax1.set_title('Segment Distribution', fontweight='bold')\n",
        "        ax1.set_xlabel('Segment')\n",
        "        ax1.set_ylabel('Count')\n",
        "\n",
        "        # 2. PCA Visualization\n",
        "        ax2 = plt.subplot(2, 3, 2)\n",
        "        scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=df['segment'], cmap='viridis', alpha=0.6)\n",
        "        ax2.set_title('Customer Segments (PCA)', fontweight='bold')\n",
        "        ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "        ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "        plt.colorbar(scatter, ax=ax2, label='Segment')\n",
        "\n",
        "        # 3. Churn Rate by Segment\n",
        "        ax3 = plt.subplot(2, 3, 3)\n",
        "        df.groupby('segment')['churn'].mean().plot(kind='bar', ax=ax3, color='coral')\n",
        "        ax3.set_title('Churn Rate by Segment', fontweight='bold')\n",
        "        ax3.set_xlabel('Segment')\n",
        "        ax3.set_ylabel('Churn Rate')\n",
        "        ax3.axhline(df['churn'].mean(), color='red', linestyle='--', label='Overall Avg')\n",
        "        ax3.legend()\n",
        "\n",
        "        # 4. Tenure by Segment\n",
        "        ax4 = plt.subplot(2, 3, 4)\n",
        "        df.boxplot(column='tenure_months', by='segment', ax=ax4)\n",
        "        ax4.set_title('Tenure Distribution by Segment')\n",
        "        ax4.set_xlabel('Segment')\n",
        "        ax4.set_ylabel('Tenure (months)')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 5. Monthly Charges by Segment\n",
        "        ax5 = plt.subplot(2, 3, 5)\n",
        "        df.boxplot(column='monthly_charges', by='segment', ax=ax5)\n",
        "        ax5.set_title('Monthly Charges by Segment')\n",
        "        ax5.set_xlabel('Segment')\n",
        "        ax5.set_ylabel('Monthly Charges ($)')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        # 6. Engagement by Segment\n",
        "        ax6 = plt.subplot(2, 3, 6)\n",
        "        df.boxplot(column='login_frequency', by='segment', ax=ax6)\n",
        "        ax6.set_title('Login Frequency by Segment')\n",
        "        ax6.set_xlabel('Segment')\n",
        "        ax6.set_ylabel('Login Frequency')\n",
        "        plt.suptitle('')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "        return fig\n",
        "\n",
        "\n",
        "class ChurnPreventionEngine:\n",
        "    \"\"\"Generate personalized retention strategies.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_strategies(df: pd.DataFrame, churn_proba: np.ndarray) -> pd.DataFrame:\n",
        "        \"\"\"Create personalized prevention strategies for high-risk customers.\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CHURN PREVENTION STRATEGIES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        df = df.copy()\n",
        "        df['churn_probability'] = churn_proba\n",
        "\n",
        "        # Identify high-risk customers (top 20% churn probability)\n",
        "        high_risk_threshold = df['churn_probability'].quantile(0.80)\n",
        "        high_risk_customers = df[df['churn_probability'] >= high_risk_threshold].copy()\n",
        "\n",
        "        print(f\"\\nIdentified {len(high_risk_customers)} high-risk customers\")\n",
        "        print(f\"(Churn probability >= {high_risk_threshold:.2%})\")\n",
        "\n",
        "        # Generate personalized strategies\n",
        "        strategies = []\n",
        "\n",
        "        for _, customer in high_risk_customers.iterrows():\n",
        "            strategy = {\n",
        "                'customer_id': customer['customer_id'],\n",
        "                'churn_probability': customer['churn_probability'],\n",
        "                'segment': customer.get('segment', 'Unknown'),\n",
        "                'primary_issues': [],\n",
        "                'recommended_actions': [],\n",
        "                'priority': 'High' if customer['churn_probability'] > 0.8 else 'Medium'\n",
        "            }\n",
        "\n",
        "            # Identify issues\n",
        "            if customer.get('support_tickets', 0) > 3:\n",
        "                strategy['primary_issues'].append('High support tickets')\n",
        "                strategy['recommended_actions'].append('Priority support escalation')\n",
        "\n",
        "            if customer.get('complaints', 0) > 2:\n",
        "                strategy['primary_issues'].append('Multiple complaints')\n",
        "                strategy['recommended_actions'].append('Personal outreach from account manager')\n",
        "\n",
        "            if customer.get('payment_delay_days', 0) > 10:\n",
        "                strategy['primary_issues'].append('Payment delays')\n",
        "                strategy['recommended_actions'].append('Flexible payment plan offer')\n",
        "\n",
        "            if customer.get('login_frequency', 30) < 10:\n",
        "                strategy['primary_issues'].append('Low engagement')\n",
        "                strategy['recommended_actions'].append('Re-engagement campaign with tutorial')\n",
        "\n",
        "            if customer.get('days_since_last_login', 0) > 20:\n",
        "                strategy['primary_issues'].append('Inactive account')\n",
        "                strategy['recommended_actions'].append('Win-back offer with discount')\n",
        "\n",
        "            if customer.get('contract_type', '') == 'Month-to-Month':\n",
        "                strategy['primary_issues'].append('No long-term commitment')\n",
        "                strategy['recommended_actions'].append('Annual contract upgrade incentive')\n",
        "\n",
        "            if customer.get('monthly_charges', 50) > 100:\n",
        "                strategy['primary_issues'].append('High price point')\n",
        "                strategy['recommended_actions'].append('Customized package review')\n",
        "\n",
        "            if not strategy['primary_issues']:\n",
        "                strategy['primary_issues'].append('General churn risk')\n",
        "                strategy['recommended_actions'].append('Customer satisfaction survey')\n",
        "\n",
        "            strategies.append(strategy)\n",
        "\n",
        "        # Create DataFrame\n",
        "        strategies_df = pd.DataFrame(strategies)\n",
        "\n",
        "        # Display sample strategies\n",
        "        print(\"\\nSample Prevention Strategies:\")\n",
        "        print(\"=\"*70)\n",
        "        for i, strategy in enumerate(strategies[:3], 1):\n",
        "            print(f\"\\nCustomer {i}: {strategy['customer_id']}\")\n",
        "            print(f\"  Churn Probability: {strategy['churn_probability']:.1%}\")\n",
        "            print(f\"  Priority: {strategy['priority']}\")\n",
        "            print(f\"  Issues: {', '.join(strategy['primary_issues'])}\")\n",
        "            print(f\"  Actions: {'; '.join(strategy['recommended_actions'])}\")\n",
        "\n",
        "        return strategies_df\n",
        "\n",
        "\n",
        "class ModelPipeline:\n",
        "    \"\"\"Complete ML pipeline for training and inference.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.preprocessor = DataPreprocessor()\n",
        "        self.model = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def train_pipeline(self, df: pd.DataFrame):\n",
        "        \"\"\"Train the complete pipeline.\"\"\"\n",
        "\n",
        "        # Clean data\n",
        "        df_clean = self.preprocessor.clean_data(df)\n",
        "\n",
        "        # Engineer features\n",
        "        df_features = self.preprocessor.engineer_features(df_clean)\n",
        "\n",
        "        # Encode categorical\n",
        "        df_encoded = self.preprocessor.encode_categorical(df_features, fit=True)\n",
        "\n",
        "        # Prepare features\n",
        "        X, y = self.preprocessor.prepare_features(df_encoded, fit=True)\n",
        "\n",
        "        self.feature_names = self.preprocessor.feature_names\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def predict_pipeline(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Inference pipeline for new data.\"\"\"\n",
        "\n",
        "        # Clean data\n",
        "        df_clean = self.preprocessor.clean_data(df)\n",
        "\n",
        "        # Engineer features\n",
        "        df_features = self.preprocessor.engineer_features(df_clean)\n",
        "\n",
        "        # Encode categorical\n",
        "        df_encoded = self.preprocessor.encode_categorical(df_features, fit=False)\n",
        "\n",
        "        # Prepare features\n",
        "        X, _ = self.preprocessor.prepare_features(df_encoded, fit=False)\n",
        "\n",
        "        # Predict\n",
        "        predictions = self.model.predict_proba(X)[:, 1]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def save_pipeline(self, model, filepath: str):\n",
        "        \"\"\"Save the complete pipeline.\"\"\"\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        pipeline_data = {\n",
        "            'model': model,\n",
        "            'preprocessor': self.preprocessor,\n",
        "            'feature_names': self.feature_names\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(pipeline_data, f)\n",
        "\n",
        "        print(f\"\\n✓ Pipeline saved to {filepath}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load_pipeline(filepath: str):\n",
        "        \"\"\"Load a saved pipeline.\"\"\"\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            pipeline_data = pickle.load(f)\n",
        "\n",
        "        pipeline = ModelPipeline()\n",
        "        pipeline.model = pipeline_data['model']\n",
        "        pipeline.preprocessor = pipeline_data['preprocessor']\n",
        "        pipeline.feature_names = pipeline_data['feature_names']\n",
        "\n",
        "        print(f\"✓ Pipeline loaded from {filepath}\")\n",
        "\n",
        "        return pipeline\n",
        "\n",
        "def main():\n",
        "    \"\"\"Execute the complete churn prediction system.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CUSTOMER CHURN PREDICTION & BEHAVIORAL ANALYTICS SYSTEM\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"A comprehensive end-to-end machine learning solution\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Create output directory\n",
        "    import os\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "    print(\"\\n[STEP 1] Generating Customer Data...\")\n",
        "    data_generator = CustomerDataGenerator(n_samples=10000, random_state=42)\n",
        "    df_raw = data_generator.generate_data()\n",
        "    df_raw.to_csv('outputs/raw_customer_data.csv', index=False)\n",
        "    print(f\"✓ Generated {len(df_raw)} customer records\")\n",
        "\n",
        "    print(\"\\n[STEP 2-3] Data Preprocessing and Feature Engineering...\")\n",
        "    preprocessor = DataPreprocessor()\n",
        "    df_clean = preprocessor.clean_data(df_raw)\n",
        "    df_features = preprocessor.engineer_features(df_clean)\n",
        "    df_processed = preprocessor.encode_categorical(df_features, fit=True)\n",
        "    df_processed.to_csv('outputs/processed_customer_data.csv', index=False)\n",
        "\n",
        "    print(\"\\n[STEP 4] Exploratory Data Analysis...\")\n",
        "    eda = ExploratoryAnalysis()\n",
        "    summary = eda.generate_statistical_summary(df_features)\n",
        "    summary.to_csv('outputs/statistical_summary.csv')\n",
        "\n",
        "    fig_eda = eda.plot_churn_analysis(df_features, save_path='outputs/eda_visualizations.png')\n",
        "    plt.close(fig_eda)\n",
        "    print(\"✓ EDA visualizations saved\")\n",
        "\n",
        "    # Prepare data for modeling\n",
        "    X, y = preprocessor.prepare_features(df_processed, fit=True)\n",
        "    feature_names = preprocessor.feature_names\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"\\n[STEP 5] Handling Class Imbalance...\")\n",
        "    model_trainer = ChurnPredictionModels()\n",
        "    X_train_balanced, y_train_balanced = model_trainer.handle_class_imbalance(\n",
        "        X_train, y_train, method='smote'\n",
        "    )\n",
        "\n",
        "    print(\"\\n[STEP 6-8] Training Multiple ML Models...\")\n",
        "    model_trainer.train_models(X_train_balanced, y_train_balanced, X_test, y_test)\n",
        "\n",
        "    # Store y_test in results for plotting\n",
        "    for model_name in model_trainer.results:\n",
        "        model_trainer.results[model_name]['y_true'] = y_test\n",
        "\n",
        "    # Plot model comparison\n",
        "    fig_models = model_trainer.plot_model_comparison(save_path='outputs/model_comparison.png')\n",
        "    plt.close(fig_models)\n",
        "    print(\"✓ Model comparison saved\")\n",
        "\n",
        "    # Select best model\n",
        "    best_model_name, best_model = model_trainer.select_best_model()\n",
        "\n",
        "    print(\"\\n[STEP 7] Hyperparameter Tuning...\")\n",
        "    if best_model_name in ['Random Forest', 'XGBoost']:\n",
        "        tuned_model = model_trainer.hyperparameter_tuning(\n",
        "            X_train_balanced, y_train_balanced, model_name=best_model_name\n",
        "        )\n",
        "        if tuned_model:\n",
        "            best_model = tuned_model\n",
        "            # Re-evaluate tuned model\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "            print(f\"\\nTuned model ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "    print(\"\\n[STEP 9] Applying Explainable AI (SHAP)...\")\n",
        "    explainer = ModelExplainability(best_model, X_train_balanced, feature_names)\n",
        "    shap_values = explainer.compute_shap_values(X_test, sample_size=100)\n",
        "\n",
        "    if shap_values is not None:\n",
        "        fig_shap = explainer.plot_feature_importance(save_path='outputs/shap_feature_importance.png')\n",
        "        if fig_shap:\n",
        "            plt.close(fig_shap)\n",
        "\n",
        "    feature_importance_df = explainer.get_feature_importance_scores(top_n=15)\n",
        "    if feature_importance_df is not None:\n",
        "        feature_importance_df.to_csv('outputs/feature_importance.csv', index=False)\n",
        "\n",
        "    print(\"\\n[STEP 10] Customer Segmentation...\")\n",
        "    segmenter = CustomerSegmentation(n_clusters=4)\n",
        "    cluster_features = ['tenure_months', 'monthly_charges', 'login_frequency',\n",
        "                       'support_tickets', 'complaints', 'payment_delay_days']\n",
        "    df_segmented = segmenter.perform_segmentation(df_features, cluster_features)\n",
        "\n",
        "    fig_segments = segmenter.visualize_segments(df_segmented, save_path='outputs/customer_segments.png')\n",
        "    plt.close(fig_segments)\n",
        "\n",
        "    df_segmented.to_csv('outputs/segmented_customers.csv', index=False)\n",
        "    print(\"✓ Customer segmentation completed\")\n",
        "\n",
        "    print(\"\\n[STEP 11] Generating Churn Prevention Strategies...\")\n",
        "    churn_proba = best_model.predict_proba(X)[:, 1]\n",
        "    df_segmented['churn_probability'] = churn_proba\n",
        "\n",
        "    prevention_engine = ChurnPreventionEngine()\n",
        "    strategies_df = prevention_engine.generate_strategies(df_segmented, churn_proba)\n",
        "    strategies_df.to_csv('outputs/churn_prevention_strategies.csv', index=False)\n",
        "    print(\"✓ Prevention strategies generated\")\n",
        "\n",
        "    print(\"\\n[STEP 12] Saving Models and Pipelines...\")\n",
        "    pipeline = ModelPipeline()\n",
        "    pipeline.preprocessor = preprocessor\n",
        "    pipeline.feature_names = feature_names\n",
        "    pipeline.save_pipeline(best_model, 'outputs/churn_prediction_pipeline.pkl')\n",
        "\n",
        "    # Save all models\n",
        "    with open('outputs/all_trained_models.pkl', 'wb') as f:\n",
        "        pickle.dump(model_trainer.models, f)\n",
        "    print(\"✓ All models saved\")\n",
        "\n",
        "    print(\"\\n[STEP 13] Testing Inference Pipeline...\")\n",
        "\n",
        "    # Load pipeline\n",
        "    loaded_pipeline = ModelPipeline.load_pipeline('outputs/churn_prediction_pipeline.pkl')\n",
        "\n",
        "    # Test on new data (using a small sample from raw data)\n",
        "    new_customers = df_raw.sample(10, random_state=42).copy()\n",
        "    new_customers = new_customers.drop('churn', axis=1)  # Remove target\n",
        "\n",
        "    predictions = loaded_pipeline.predict_pipeline(new_customers)\n",
        "\n",
        "    new_customers['churn_probability'] = predictions\n",
        "    new_customers['risk_level'] = pd.cut(predictions,\n",
        "                                         bins=[0, 0.3, 0.6, 1.0],\n",
        "                                         labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "    print(\"\\nSample Predictions on New Customers:\")\n",
        "    print(new_customers[['customer_id', 'tenure_months', 'monthly_charges',\n",
        "                         'churn_probability', 'risk_level']].to_string(index=False))\n",
        "\n",
        "    new_customers.to_csv('outputs/sample_predictions.csv', index=False)\n",
        "\n",
        "    print(\"\\n[STEP 14] Generating Business Insights Report...\")\n",
        "\n",
        "    insights = {\n",
        "        'executive_summary': {\n",
        "            'total_customers': len(df_raw),\n",
        "            'overall_churn_rate': f\"{df_raw['churn'].mean() * 100:.2f}%\",\n",
        "            'high_risk_customers': len(strategies_df),\n",
        "            'best_model': best_model_name,\n",
        "            'model_accuracy': f\"{model_trainer.results[best_model_name]['accuracy']:.2%}\",\n",
        "            'model_roc_auc': f\"{model_trainer.results[best_model_name]['roc_auc']:.4f}\"\n",
        "        },\n",
        "        'key_churn_drivers': feature_importance_df.to_dict('records') if feature_importance_df is not None else [],\n",
        "        'segment_insights': segmenter.segment_profiles.to_dict() if segmenter.segment_profiles is not None else {},\n",
        "        'recommendations': [\n",
        "            \"1. Focus retention efforts on month-to-month contract customers\",\n",
        "            \"2. Proactively address customers with 2+ support tickets\",\n",
        "            \"3. Implement re-engagement campaigns for low-activity users\",\n",
        "            \"4. Offer payment flexibility to customers with payment delays\",\n",
        "            \"5. Provide personalized upgrade paths to high-value segments\",\n",
        "            \"6. Monitor engagement decay patterns weekly\",\n",
        "            \"7. Establish early warning system for customers inactive >14 days\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with open('outputs/business_insights_report.json', 'w') as f:\n",
        "        json.dump(insights, f, indent=2, default=str)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SYSTEM EXECUTION COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nOutputs Generated:\")\n",
        "    print(\"  1. raw_customer_data.csv - Original dataset\")\n",
        "    print(\"  2. processed_customer_data.csv - Cleaned & engineered features\")\n",
        "    print(\"  3. statistical_summary.csv - Comprehensive EDA summary\")\n",
        "    print(\"  4. eda_visualizations.png - Exploratory visualizations\")\n",
        "    print(\"  5. model_comparison.png - Model performance comparison\")\n",
        "    print(\"  6. shap_feature_importance.png - Feature importance analysis\")\n",
        "    print(\"  7. feature_importance.csv - Top churn predictors\")\n",
        "    print(\"  8. customer_segments.png - Segmentation visualizations\")\n",
        "    print(\"  9. segmented_customers.csv - Customers with segments\")\n",
        "    print(\"  10. churn_prevention_strategies.csv - Actionable strategies\")\n",
        "    print(\"  11. churn_prediction_pipeline.pkl - Production-ready pipeline\")\n",
        "    print(\"  12. all_trained_models.pkl - All trained models\")\n",
        "    print(\"  13. sample_predictions.csv - Inference demo results\")\n",
        "    print(\"  14. business_insights_report.json - Executive summary\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run main system\n",
        "    insights = main()"
      ]
    }
  ]
}